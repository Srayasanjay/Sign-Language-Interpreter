# Sign-Language-Detector
This project is a learning experiment to understand how datasets are created, structured, and used for training machine learning models. It uses OpenCV and MediaPipe to capture and process hand landmarks from videos, converting them into numerical data that can later be used for gesture/sign recognition.

I started this project because I wanted to learn how datasets are built and see how a machine “learns” from structured data. This helped me understand:
1. How raw video input is transformed into features (numerical representation).
2. How data preprocessing and organization form the backbone of ML models.
3. The importance of clean, labeled datasets in supervised learning.

## Technologies Used
1. Python
2. OpenCV – for video processing
3. MediaPipe – for hand landmark detection
4. NumPy – for storing structured landmark data
5. Google Colab – for running and testing the pipeline

Here is the colab notebook:
Model training -> https://colab.research.google.com/drive/1q2vDgMEVDMAond6_k2CEnKw_CKb3fdQz?usp=sharing
